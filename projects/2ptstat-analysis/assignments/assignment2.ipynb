{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this assignment we learn how to derive a neural network emultor based on the cosmopower emulator (Mancini et al. 2021). Based on this emualtor we perform a PCA data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment you need the install cosmopower yourself https://github.com/alessiospuriomancini/cosmopower or you just use the cosmopower_NN.py module that is provided. However in the latter case you need to install tensorflow: https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from cosmopower_NN import cosmopower_NN\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: load in the all the 10000 model vectors and corresponding paramters that we use to bulit the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.load('data_4_assignment2/models.npy') \n",
    "parameters = np.load('data_4_assignment2/parameters.npz')\n",
    "parameters.keys(),models.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the models and parameters in a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = ... # select which elements you wanna use for training\n",
    "test_sample = ... # select which elements you wanna use for training\n",
    "\n",
    "train_params = {}\n",
    "for name in parameters.keys():\n",
    "    train_params[name]=list(np.array(parameters[name])[train_sample])\n",
    "    \n",
    "test_params = {}\n",
    "for name in parameters.keys():\n",
    "    test_params[name]=list(np.array(parameters[name])[test_sample])\n",
    "\n",
    "train_features = np.load('data_4_assignment2/models.npy')[train_sample]\n",
    "test_features = np.load('data_4_assignment2/models.npy')[test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the emulator. Reasonable hyper parameres are set already, but test out other settings and comment on what you observe regarding accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These premodifications are not necessary but might improve the accuracy\n",
    "#Here you can try to modify the training features in advance \n",
    "features_modified=train_features\n",
    "\n",
    "cp_nn = cosmopower_NN(parameters=list(parameters.keys()), \n",
    "                    modes=np.linspace(-1,1,train_features.shape[1]), \n",
    "                    n_hidden = [ 4, 5, 6], # This you should modify. In this exmaple we use three layers with 4, 5 and 6 nodes. Is this enough?\n",
    "                    verbose=True, # useful to understand the different steps in initialisation and training\n",
    "                    )\n",
    "\n",
    "device = 'cpu'\n",
    "with tf.device(device):\n",
    "    # train\n",
    "    cp_nn.train(training_parameters=train_params,\n",
    "                training_features=features_modified,\n",
    "                filename_saved_model='data_4_assignment2/emulator_test', # the name of the emulator and where to save it\n",
    "                # cooling schedule\n",
    "                validation_split=0.1, # The precentage from train sample that is used for the validation\n",
    "                learning_rates=[1e-2, 1e-3], # the different leanring rates. This need to be adjusted\n",
    "                batch_sizes=[100, 100], # the number for models that are used to adjust the NN parameters\n",
    "                patience_values = [100, 100], # Number of epoch to wait before decreasing the learning rate if the loss does not improve anymore\n",
    "                max_epochs = [200,200], # Maxmimal number of epoch before decreasing the learning rate\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy of the emulator, by computing how many predictions are inside 68%, 95%, 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulated_features = cp_nn.predictions_np(test_params)\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "\n",
    "diff=(emulated_features/test_features-1) # Measure the relative difference between test sample and the emualted models\n",
    "\n",
    "mean_diff = ... \n",
    "percentiles1 = np.percentile(...) \n",
    "percentiles2 = ... \n",
    "percentiles3 = ...\n",
    "bins=range(diff.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 3),dpi=100)\n",
    "plt.fill_between(...,  label = '$99\\%$', alpha=0.8)\n",
    "plt.fill_between(... label = '$95\\%$', alpha = 0.7)\n",
    "plt.fill_between(...  label = '$68\\%$', alpha = 1)\n",
    "plt.plot(bins,mean_diff,'-',color='black')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(frameon=False, fontsize=15, loc='upper right')\n",
    "plt.ylabel(r'$| m^{\\mathrm{emulated}} - m^{\\mathrm{true}}|/  m^{\\mathrm{true}}$', fontsize=15)\n",
    "plt.xlabel(r'vector elements',  fontsize=15)\n",
    "plt.ylim(-0.001,0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we want perform a Fisher Analysis using the emualtor. We want you to compute the covariance of the parameters $\\Theta = \\{\\Omega_\\mathrm{m},w\\}$, which can be estimated by $C(\\Theta) = F^{-1}$, where $$F_{ij}= \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)^{T} C^{-1} \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)$$\n",
    "##### For the partial derivatives we use the five point stencil beam given by $$\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\approx \\frac{-m(\\Theta_i + 2\\ \\Delta \\Theta_i) + 8 \\ m(\\Theta_i +  \\Delta \\Theta_i) - 8 \\ m(\\Theta_i - \\Delta \\Theta_i) + m(\\Theta_i - 2\\ \\Delta \\Theta_i)}{12 \\ \\Delta \\Theta_i }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Om_shift = ... #decide for a reasonable stepsize in Omega_m\n",
    "w_shift = ... #decide for a reasonable stepsize in w\n",
    "\n",
    "# This example compute the model for two different Omega_m values.\n",
    "paramters = {'omega_m':[0.3,0.31,],'w':[-1,-1],'As':[np.mean(test_params['As'])]*2,'omega_b':[np.mean(test_params['omega_b'])]*2}\n",
    "    \n",
    "features_4_div = cp_nn.predictions_np(paramters)\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "\n",
    "derivative_Om = ... # compute derivative\n",
    "derivative_w = ... # compute derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So lets compute the Fisher matrix the corresponding covariance matrix of the paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=np.load('data_4_assignment1/covariance.npy') # We make use of the analy\n",
    "inv_cov = inv(cov)\n",
    "\n",
    "FoM_11 = ...\n",
    "FoM_22 = ...\n",
    "FoM_12 = ...\n",
    "FoM_best = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "Cov_parameter = ...\n",
    "Cov_parameter,np.sqrt(np.diag(Cov_parameter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First you need to generate the transformation from our model/data vectors to the PCA elements based on the 10k models. Then you need transform all 100k noisy data vectors from which you should then measure the a cvoavraicen on the PCA elements. Now it should get clear why you had to genereate the 100k multvariate Gaussian random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "N_pca = 900 # Decide how many PCA eigenvalues you want to use\n",
    "models = np.load('data_4_assignment2/models.npy')\n",
    "mean = np.mean(models,axis=0) # for numerical stability you should subtract the mean of each element\n",
    "pca = PCA(n_components=N_pca,svd_solver='full')\n",
    "models_pca = pca.fit_transform(models-mean) # Perform the PCA fitting. Now you can use pca for the transformation\n",
    "\n",
    "#rotate covariance matrix. If you do not how to do that. You can also create 10k random data vectors and tranform each one individual, \n",
    "# and the compute the covariance matrix from them\n",
    "rotation_matrix = pca.components_.T\n",
    "cov_pca = ...\n",
    "\n",
    "plt.imshow(cov_pca/np.outer(np.sqrt(np.diag(cov_pca)),np.sqrt(np.diag(cov_pca))),vmin=-1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase successively the number of PCA elements from which you compute the Fisher matrix. Convince yourself that if you take all possible PCA elements you converge to same contraining power as for original Fisher analysis. How many PCA elements do you need to have 10% and 1% of the constraining power as the original Fisher analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramter_constraints = []\n",
    "for N_pca in np.arange(1,900):\n",
    "    \n",
    "    inv_cov_pca = ... # select the first N_pca elements\n",
    "\n",
    "    features_4_div_pca = pca.transform(...) # transform  the dverative and select the first N_pca elements\n",
    "\n",
    "\n",
    "    derivative_Om_pca = ... # compute derivative\n",
    "    derivative_w_pca = ... # compute derivative\n",
    "\n",
    "    FoM_11 = ...\n",
    "    FoM_22 = ...\n",
    "    FoM_12 = ...\n",
    "    FoM_best_pca = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "    Cov_parameter_pca = inv(FoM_best_pca)\n",
    "    paramter_constraints.append(np.sqrt(np.diag(Cov_parameter_pca)))\n",
    "paramter_constraints = np.array(paramter_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the constraints depending on the number of PCA that you have used. How many PCA elements do you need to get 10% and 1% of the maximum constraining power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
